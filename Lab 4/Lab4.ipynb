{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xakcl3UMtRs"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Download required NLTK data files (only the first time)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP5gwhrVU-rW",
        "outputId": "d07e6746-f129-4253-8fa1-c7a55eb3a650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy's small English model for advanced NLP tasks (make sure it's installed via: python -m spacy download en_core_web_sm)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text to process\n",
        "text = (\n",
        "    \"Apple Inc. is looking at buying U.K. startup for $1 billion. \"\n",
        "    \"This is an exciting development in the tech industry!\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "kB8_JSxjVDaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 1. Text Normalization\n",
        "# --------------------------\n",
        "# Convert the text to lowercase to standardize it\n",
        "text_lower = text.lower()\n",
        "print(\"Lowercased Text:\")\n",
        "print(text_lower, \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk-Xu8zmVIzM",
        "outputId": "71244e9a-f5c3-479f-fad3-da342e734272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased Text:\n",
            "apple inc. is looking at buying u.k. startup for $1 billion. this is an exciting development in the tech industry! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 2. Tokenization\n",
        "# --------------------------\n",
        "# Tokenize the lowercased text into words using NLTK\n",
        "tokens = word_tokenize(text_lower)\n",
        "print(\"Tokenized Words:\")\n",
        "print(tokens, \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQGvTLkAVL0M",
        "outputId": "44cb4587-3036-43f8-d155-7e59eb60d094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Words:\n",
            "['apple', 'inc.', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '$', '1', 'billion', '.', 'this', 'is', 'an', 'exciting', 'development', 'in', 'the', 'tech', 'industry', '!'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 3. Punctuation Removal\n",
        "# --------------------------\n",
        "# Remove punctuation from the token list using Python's string punctuation set\n",
        "tokens_no_punct = [token for token in tokens if token not in string.punctuation]\n",
        "print(\"Tokens without Punctuation:\")\n",
        "print(tokens_no_punct, \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xc40Z30VQWo",
        "outputId": "4fa8fa63-10d8-4d4c-c4cb-c6134325e26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without Punctuation:\n",
            "['apple', 'inc.', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '1', 'billion', 'this', 'is', 'an', 'exciting', 'development', 'in', 'the', 'tech', 'industry'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 4. Stopword Removal\n",
        "# --------------------------\n",
        "# Get the set of English stopwords from NLTK and filter them out\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stopwords = [token for token in tokens_no_punct if token not in stop_words]\n",
        "print(\"Tokens without Stopwords:\")\n",
        "print(tokens_no_stopwords, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX6jclDZVSuB",
        "outputId": "e3ec761a-7e85-4af5-ab40-47cd1fac78e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without Stopwords:\n",
            "['apple', 'inc.', 'looking', 'buying', 'u.k.', 'startup', '1', 'billion', 'exciting', 'development', 'tech', 'industry'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------------------\n",
        "# 5. Stemming\n",
        "# --------------------------\n",
        "# Use NLTK's PorterStemmer to reduce words to their stem (root form)\n",
        "ps = PorterStemmer()\n",
        "stemmed_tokens = [ps.stem(token) for token in tokens_no_stopwords]\n",
        "print(\"Stemmed Tokens:\")\n",
        "print(stemmed_tokens, \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49SAwpcdVVrZ",
        "outputId": "6cba542d-4ca5-491d-e330-043ad54f0114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens:\n",
            "['appl', 'inc.', 'look', 'buy', 'u.k.', 'startup', '1', 'billion', 'excit', 'develop', 'tech', 'industri'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 6. Part-of-Speech (POS) Tagging\n",
        "# --------------------------\n",
        "# Tag each token (without punctuation and stopwords) with its part-of-speech\n",
        "pos_tags = nltk.pos_tag(tokens_no_stopwords)\n",
        "print(\"POS Tags:\")\n",
        "print(pos_tags, \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e_VMZOkVXz2",
        "outputId": "e0becdfd-0c77-4aac-bf66-179dca9e93f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags:\n",
            "[('apple', 'NN'), ('inc.', 'NN'), ('looking', 'VBG'), ('buying', 'VBG'), ('u.k.', 'JJ'), ('startup', 'JJ'), ('1', 'CD'), ('billion', 'CD'), ('exciting', 'VBG'), ('development', 'NN'), ('tech', 'NN'), ('industry', 'NN')] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 7. Named Entity Recognition (NER) and Dependency Parsing with spaCy\n",
        "# --------------------------\n",
        "# Process the original text with spaCy's pipeline\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print Named Entities found in the text\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")\n",
        "print()\n",
        "\n",
        "# Print Dependency Parsing information for each token in the sentence\n",
        "print(\"Dependency Parsing:\")\n",
        "for token in doc:\n",
        "    # token.dep_ is the dependency relation, token.head is the parent word\n",
        "    print(f\"{token.text:12} {token.dep_:10} -> {token.head.text}\")\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymCYt68BVaV-",
        "outputId": "8efcadd0-2ed3-40f4-a0ca-65edbe14febc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "Apple Inc. -> ORG\n",
            "U.K. -> GPE\n",
            "$1 billion -> MONEY\n",
            "\n",
            "Dependency Parsing:\n",
            "Apple        compound   -> Inc.\n",
            "Inc.         nsubj      -> looking\n",
            "is           aux        -> looking\n",
            "looking      ROOT       -> looking\n",
            "at           prep       -> looking\n",
            "buying       pcomp      -> at\n",
            "U.K.         dobj       -> buying\n",
            "startup      dep        -> looking\n",
            "for          prep       -> startup\n",
            "$            quantmod   -> billion\n",
            "1            compound   -> billion\n",
            "billion      pobj       -> for\n",
            ".            punct      -> looking\n",
            "This         nsubj      -> is\n",
            "is           ROOT       -> is\n",
            "an           det        -> development\n",
            "exciting     amod       -> development\n",
            "development  attr       -> is\n",
            "in           prep       -> development\n",
            "the          det        -> industry\n",
            "tech         compound   -> industry\n",
            "industry     pobj       -> in\n",
            "!            punct      -> is\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------------------\n",
        "# 8. Sentiment Analysis using TextBlob\n",
        "# --------------------------\n",
        "# Create a TextBlob object and analyze sentiment\n",
        "blob = TextBlob(text)\n",
        "print(\"Sentiment Analysis:\")\n",
        "print(\"Polarity:\", blob.sentiment.polarity)      # Ranges from -1 (negative) to 1 (positive)\n",
        "print(\"Subjectivity:\", blob.sentiment.subjectivity)  # Ranges from 0 (objective) to 1 (subjective)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvlE5sUJVbeK",
        "outputId": "e378b107-dd56-4a79-ff10-4cf88aeccac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis:\n",
            "Polarity: 0.375\n",
            "Subjectivity: 0.8\n"
          ]
        }
      ]
    }
  ]
}